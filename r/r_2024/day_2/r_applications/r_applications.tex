\documentclass[serif, 9pt, aspectratio=32]{beamer} 
\usetheme{Darmstadt}

\usepackage{appendixnumberbeamer}
\usepackage{booktabs}
\usepackage[scale=2]{ccicons}
\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}
\usepackage{xspace}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usetikzlibrary{shapes, arrows}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}

\title{R Applications}
\date{\today}
\author{Tan Sein Jone}
\institute{University of British Columbia}

\pgfplotsset{compat=1.18}
\setbeamertemplate{footline}[frame number]

\begin{document}

\maketitle

\begin{frame}{Table of contents}
    \setbeamertemplate{section in toc}[sections numbered]
    \tableofcontents[hideallsubsections]
\end{frame}

\section{Piping}

\begin{frame}
    \frametitle{Table of Contents}
    \setbeamertemplate{section in toc}[sections numbered]
    \tableofcontents[currentsection]
\end{frame}

\begin{frame}{Piping}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item Piping is a way to simplify your code by passing the output of one function directly to the input of another function.
        \item Piping is done using the pipe operator, which is represented by the \%\textgreater\% symbol.
        \item Piping makes your code more readable and easier to understand by breaking it down into smaller, more manageable steps.
    \end{itemize}
\end{frame}

\begin{frame}{Piping}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item Piping is especially useful when you have a series of operations that need to be performed on the same data.
        \item Piping allows you to chain together multiple functions in a single line of code.
        \item Piping is a powerful tool that can help you write more efficient and concise code.
    \end{itemize}
\end{frame}

\begin{frame}{Piping}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item Piping is done using the pipe operator, which is represented by the \%\textgreater\% symbol.
        \item The pipe operator takes the output of the function on the left and passes it as the first argument to the function on the right.
        \item The pipe operator can be used to chain together multiple functions in a single line of code.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Example of Piping vs. Non-Piping}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item \textbf{Without Piping:}
              \begin{lstlisting}[language=R]
            # Load necessary library
            library(dplyr)
            
            # Generate data
            set.seed(123)
            data <- data.frame(
              x = rnorm(100, mean = 5, sd = 2),
              y = rnorm(100, mean = 6, sd = 2)
            )
            
            # Non-piping approach
            data <- mutate(data, z = x + y)
            data <- filter(data, z > 10)
            summary(data)
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Example of Piping vs. Non-Piping}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item \textbf{Without Piping:}
              \begin{lstlisting}[language=R]
            # Load necessary library
            library(dplyr)
            
            # Generate data
            set.seed(123)
            data <- data.frame(
              x = rnorm(100, mean = 5, sd = 2),
              y = rnorm(100, mean = 6, sd = 2)
            )
            
            # Piping approach
            data %>%
              mutate(z = x + y) %>%
              filter(z > 10) %>%
              summary()
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Breakdown of Piping}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item \textbf{Load Necessary Library:}
              \begin{itemize}
                  \item \texttt{library(dplyr)}: Loads the \texttt{dplyr} package, which provides functions for data manipulation.
              \end{itemize}

        \item \textbf{Generate Data:}
              \begin{itemize}
                  \item \texttt{set.seed(123)}: Sets the seed for random number generation to ensure reproducibility of results.
                  \item \texttt{data <- data.frame(x = rnorm(100, mean = 5, sd = 2), y = rnorm(100, mean = 6, sd = 2)}: Generates 100 random numbers from a normal distribution with specified means and standard deviations, and assigns them to variables \texttt{x} and \texttt{y}.
              \end{itemize}

        \item \textbf{Non-Piping Approach:}
              \begin{itemize}
                  \item \texttt{data <- mutate(data, z = x + y)}: Adds a new variable \texttt{z} to the data frame by summing variables \texttt{x} and \texttt{y}.
                  \item \texttt{data <- filter(data, z > 10)}: Filters the data frame to include only rows where variable \texttt{z} is greater than 10.
                  \item \texttt{summary(data)}: Displays a summary of the data frame.
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Tips for Piping}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item Use piping to chain together multiple functions in a single line of code.
        \item Use piping to break down complex operations into smaller, more manageable steps.
        \item Use piping to make your code more readable and easier to understand.
        \item Use piping to improve the efficiency and conciseness of your code.
    \end{itemize}
\end{frame}

\section{Hypothesis Testing}

\begin{frame}
    \frametitle{Table of Contents}
    \setbeamertemplate{section in toc}[sections numbered]
    \tableofcontents[currentsection]
\end{frame}

\begin{frame}{Hypothesis Testing}
    \begin{itemize}
        \item Hypothesis testing is a statistical method that is used in making statistical decisions using experimental data.
        \item A hypothesis test evaluates two mutually exclusive statements about a population to determine which statement is best supported by the sample data.
        \item These two statements are called the null hypothesis and the alternative hypothesis.
    \end{itemize}
\end{frame}

\begin{frame}{Hypothesis Testing}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item The null hypothesis is the statement being tested. Usually, the null hypothesis states that there is no effect or no difference.
        \item The alternative hypothesis is the statement that is accepted if the sample data provide enough evidence that the null hypothesis is false.
        \item The alternative hypothesis states that there is an effect or a difference.
    \end{itemize}
\end{frame}

\begin{frame}{Hypothesis Testing}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item The hypothesis test is conducted by comparing the value of the test statistic to a critical value.
        \item The critical value is a value that determines whether the null hypothesis can be rejected.
        \item If the test statistic is more extreme than the critical value, then the null hypothesis is rejected.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Types of Hypothesis Tests}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item One-Sample t-Test: Used to test whether the mean of a single sample is significantly different from a specified value.
        \item Two-Sample t-Test: Used to test whether the means of two independent samples are significantly different from each other.
        \item Paired t-Test: Used to test whether the means of two paired samples are significantly different from each other.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Types of Hypothesis Tests}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item One-Sample z-Test: Used to test whether the mean of a single sample is significantly different from a specified value when the sample size is large.
        \item Two-Sample z-Test: Used to test whether the means of two independent samples are significantly different from each other when the sample sizes are large.
        \item Chi-Square Test: Used to test whether the observed frequencies in a contingency table are significantly different from the expected frequencies.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Types of Hypothesis Tests}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item One-Way ANOVA: Used to test whether the means of three or more independent samples are significantly different from each other.
        \item Two-Way ANOVA: Used to test whether the means of two or more independent samples are significantly different from each other, taking into account two independent variables.
        \item Goodness of Fit Test: Used to test whether the observed frequencies in a sample are consistent with the expected frequencies.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Types of Hypothesis Tests}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item Test for Equal Variances: Used to test whether the variances of two or more samples are equal.
        \item Test for Normality: Used to test whether the data in a sample comes from a normal distribution.
        \item Test for Independence: Used to test whether the observations in a sample are independent of each other.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Steps in Hypothesis Testing}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item Step 1: State the null hypothesis and the alternative hypothesis.
        \item Step 2: Choose the significance level.
        \item Step 3: Collect the sample data and calculate the test statistic.
        \item Step 4: Determine the critical value or the p-value.
        \item Step 5: Make a decision to reject or fail to reject the null hypothesis.
        \item Step 6: Interpret the results of the hypothesis test.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Null and Alternative Hypotheses}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item The null hypothesis is denoted by $H_0$ and states that there is no effect or no difference.
        \item The alternative hypothesis is denoted by $H_1$ and states that there is an effect or a difference.
        \item The null hypothesis is the default assumption that is tested against the alternative hypothesis.
        \item The null hypothesis is rejected if the sample data provide enough evidence that the null hypothesis is false.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Assumptions of Hypothesis Testing}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item The sample data are independent and identically distributed.
        \item The sample data are drawn from a population that is normally distributed.
        \item The sample data are drawn from a population that has a constant variance.
        \item The sample data are drawn from a population that is randomly selected.
        \item The sample data are drawn from a population that is representative of the population of interest.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Significance Level}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item The significance level is the probability of rejecting the null hypothesis when it is true.
        \item The significance level is denoted by $\alpha$ and is usually set to 0.05.
        \item The significance level is the threshold for determining whether the null hypothesis should be rejected.
        \item If the p-value is less than the significance level, then the null hypothesis is rejected.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Test Statistic}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item The test statistic is a numerical value that is used to determine whether the null hypothesis should be rejected.
        \item The test statistic is calculated from the sample data and is compared to a critical value or a p-value.
        \item The test statistic measures the strength of the evidence against the null hypothesis.
        \item The test statistic is used to make a decision to reject or fail to reject the null hypothesis.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Critical Value}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item The critical value is a value that determines whether the null hypothesis can be rejected.
        \item The critical value is determined by the significance level and the degrees of freedom.
        \item The critical value is compared to the test statistic to determine whether the null hypothesis should be rejected.
        \item If the test statistic is more extreme than the critical value, then the null hypothesis is rejected.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{P-Value}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item The p-value is the probability of observing a test statistic as extreme as the one computed from the sample data, assuming that the null hypothesis is true.
        \item The p-value is a measure of the strength of the evidence against the null hypothesis.
        \item The p-value is compared to the significance level to determine whether the null hypothesis should be rejected.
        \item If the p-value is less than the significance level, then the null hypothesis is rejected.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Type I and Type II Errors}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item Type I Error: Occurs when the null hypothesis is rejected when it is true.
        \item Type II Error: Occurs when the null hypothesis is not rejected when it is false.
        \item The probability of a Type I Error is denoted by $\alpha$ and is equal to the significance level.
        \item The probability of a Type II Error is denoted by $\beta$ and is equal to 1 minus the power of the test.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Power of the Test}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item The power of the test is the probability of rejecting the null hypothesis when it is false.
        \item The power of the test is equal to 1 minus the probability of a Type II Error.
        \item The power of the test is a measure of the ability of the test to detect an effect or a difference when it exists.
        \item The power of the test is affected by the sample size, the effect size, and the significance level.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Confidence Interval}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item A confidence interval is a range of values that is likely to contain the true value of the population parameter.
        \item A confidence interval is calculated from the sample data and is used to estimate the population parameter.
        \item The confidence interval is calculated from the sample mean and the standard error of the mean.
        \item The confidence interval is used to make inferences about the population parameter with a specified level of confidence.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Confidence Level}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item The confidence level is the probability that the confidence interval contains the true value of the population parameter.
        \item The confidence level is denoted by $(1 - \alpha) \times 100\%$ and is usually set to 95\%.
        \item The confidence level is the proportion of confidence intervals that contain the true value of the population parameter.
        \item The confidence level is used to make inferences about the population parameter with a specified level of confidence.
    \end{itemize}
\end{frame}

\begin{frame}{Hypothesis Testing in R}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item In R, the \texttt{t.test()} function is used to perform hypothesis tests.
        \item The \texttt{t.test()} function takes in the sample data and the null hypothesis as arguments.
        \item The function returns the test statistic, the p-value, and the confidence interval.
    \end{itemize}
\end{frame}

\begin{frame}{Hypothesis Testing in R}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item The \texttt{t.test()} function can be used to perform one-sample t-tests, two-sample t-tests, and paired t-tests.
        \item The \texttt{t.test()} function can also be used to perform one-sample z-tests and two-sample z-tests.
        \item The \texttt{t.test()} function can be used to perform hypothesis tests for means, proportions, and variances.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Hypothesis Testing in R}
    \begin{lstlisting}[language=R]
# Generate some data
set.seed(123)
x <- rnorm(100, mean = 5, sd = 2)
y <- rnorm(100, mean = 6, sd = 2)

# One-sample t-test
t.test(x, mu = 0)

# Two-sample t-test
t.test(x, y)

# Paired t-test
t.test(x, y, paired = TRUE)
    \end{lstlisting}
\end{frame}

\begin{frame}{Breakdown of Hypothesis Testing in R}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item \textbf{Set Seed:}
              \begin{itemize}
                  \item \texttt{set.seed(123)}: Sets the seed for random number generation to ensure reproducibility of results.
              \end{itemize}

        \item \textbf{Generate Data:}
              \begin{itemize}
                  \item \texttt{x <- rnorm(100, mean = 5, sd = 2)}: Generates 100 random numbers from a normal distribution with a mean of 5 and a standard deviation of 2, and assigns them to vector \texttt{x}.
                  \item \texttt{y <- rnorm(100, mean = 6, sd = 2)}: Generates 100 random numbers from a normal distribution with a mean of 6 and a standard deviation of 2, and assigns them to vector \texttt{y}.
              \end{itemize}

        \item \textbf{One-Sample t-Test:}
              \begin{itemize}
                  \item \texttt{t.test(x, mu = 0)}: Performs a one-sample t-test to check if the mean of vector \texttt{x} is significantly different from 0.
              \end{itemize}

        \item \textbf{Two-Sample t-Test:}
              \begin{itemize}
                  \item \texttt{t.test(x, y)}: Performs a two-sample t-test to check if the means of vectors \texttt{x} and \texttt{y} are significantly different from each other.
              \end{itemize}

        \item \textbf{Paired t-Test:}
              \begin{itemize}
                  \item \texttt{t.test(x, y, paired = TRUE)}: Performs a paired t-test to check if the means of vectors \texttt{x} and \texttt{y} are significantly different, assuming that the samples are paired.
              \end{itemize}
    \end{itemize}
\end{frame}

\section{Regression Analysis}

\begin{frame}
    \frametitle{Table of Contents}
    \setbeamertemplate{section in toc}[sections numbered]
    \tableofcontents[currentsection]
\end{frame}

\begin{frame}{Regression Analysis}
    \begin{itemize}
        \item Regression analysis is a statistical method that is used to model the relationship between a dependent variable and one or more independent variables.
        \item The goal of regression analysis is to estimate the parameters of the regression model that best fit the data.
        \item The regression model is a mathematical equation that describes the relationship between the dependent variable and the independent variables.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Types of Regression Models}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item Linear Regression: Used to model the relationship between a continuous dependent variable and one or more continuous independent variables.
        \item Logistic Regression: Used to model the relationship between a binary dependent variable and one or more continuous independent variables.
        \item Polynomial Regression: Used to model the relationship between a continuous dependent variable and one or more continuous independent variables using polynomial functions.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Types of Regression Models}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item Ridge Regression: Used to model the relationship between a continuous dependent variable and one or more continuous independent variables with multicollinearity.
        \item Lasso Regression: Used to model the relationship between a continuous dependent variable and one or more continuous independent variables with feature selection.
        \item Elastic Net Regression: Used to model the relationship between a continuous dependent variable and one or more continuous independent variables with both ridge and lasso regularization.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Linear Regression}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item Linear regression is a regression model that assumes a linear relationship between the dependent variable and the independent variables.
        \item The linear regression model is a mathematical equation that describes the relationship between the dependent variable and the independent variables.
        \item The linear regression model is estimated using the method of least squares, which minimizes the sum of the squared differences between the observed values and the predicted values.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Linear Regression}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item The linear regression model is specified by the equation $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n + \epsilon$, where $y$ is the dependent variable, $x_1, x_2, \ldots, x_n$ are the independent variables, $\beta_0, \beta_1, \beta_2, \ldots, \beta_n$ are the coefficients of the independent variables, and $\epsilon$ is the error term.
        \item The coefficients of the independent variables are estimated using the method of least squares, which minimizes the sum of the squared differences between the observed values and the predicted values.
    \end{itemize}
\end{frame}

% \begin{frame}
%     \frametitle{Logistic Regression}
%     \begin{itemize}
%         \setlength{\itemsep}{2em}
%         \item Logistic regression is a regression model that is used when the dependent variable is binary.
%         \item The logistic regression model is a mathematical equation that describes the relationship between the dependent variable and the independent variables.
%         \item The logistic regression model is estimated using the method of maximum likelihood, which maximizes the likelihood of the observed data given the model.
%     \end{itemize}
% \end{frame}

\begin{frame}
    \frametitle{Ordinary Least Squares}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item Ordinary least squares (OLS) is a method that is used to estimate the parameters of a linear regression model.
        \item OLS minimizes the sum of the squared differences between the observed values and the predicted values.
        \item OLS is used to estimate the coefficients of the independent variables that best fit the data.
        \item OLS is used to estimate the coefficients of the independent variables that minimize the sum of the squared differences between the observed values and the predicted values.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Formula for OLS}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item The formula for OLS is $\hat{\beta} = (X^T X)^{-1} X^T y$, where $\hat{\beta}$ is the estimated coefficients of the independent variables, $X$ is the matrix of the independent variables, and $y$ is the vector of the dependent variable.
        \item The formula for OLS is derived by minimizing the sum of the squared differences between the observed values and the predicted values.
        \item The formula for OLS is used to estimate the coefficients of the independent variables that best fit the data.
    \end{itemize}
\end{frame}

% \begin{frame}
%     \frametitle{Logistic Regression}
%     \begin{itemize}
%         \setlength{\itemsep}{2em}
%         \item The logistic regression model is specified by the equation $p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n)}}$, where $p$ is the probability of the dependent variable being 1, $x_1, x_2, \ldots, x_n$ are the independent variables, $\beta_0, \beta_1, \beta_2, \ldots, \beta_n$ are the coefficients of the independent variables, and $e$ is the base of the natural logarithm.
%         \item The coefficients of the independent variables are estimated using the method of maximum likelihood, which maximizes the likelihood of the observed data given the model.
%     \end{itemize}
% \end{frame}

\begin{frame}
    \frametitle{Polynomial Regression}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item Polynomial regression is a regression model that is used when the relationship between the dependent variable and the independent variables is not linear.
        \item The polynomial regression model is a mathematical equation that describes the relationship between the dependent variable and the independent variables using polynomial functions.
        \item The polynomial regression model is estimated using the method of least squares, which minimizes the sum of the squared differences between the observed values and the predicted values.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Polynomial Regression}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item The polynomial regression model is specified by the equation $y = \beta_0 + \beta_1 x + \beta_2 x^2 + \ldots + \beta_n x^n + \epsilon$, where $y$ is the dependent variable, $x$ is the independent variable, $\beta_0, \beta_1, \beta_2, \ldots, \beta_n$ are the coefficients of the independent variables, and $\epsilon$ is the error term.
        \item The coefficients of the independent variables are estimated using the method of least squares, which minimizes the sum of the squared differences between the observed values and the predicted values.
        \item Having a polynomial of degree $n - 1$ would mean that the model wll perfectly fit the data. So, be careful of overfitting.
    \end{itemize}
\end{frame}

\begin{frame}{Regression Analysis}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item The goodness of fit of the regression model is measured using the coefficient of determination, which is the proportion of the variance in the dependent variable that is explained by the independent variables.
        \item The coefficient of determination ranges from 0 to 1, with higher values indicating a better fit. (This is your $R^2$ value)
              \begin{itemize}
                  \item A higher $R^2$ does not mean your regression is better. It just means that your model explains more of the variance in the dependent variable.
                  \item It could also be an indication of overfitting.
              \end{itemize}
        \item The significance of the regression model is tested using the F-test, which tests whether the regression model is a better fit than a model with no independent variables.
    \end{itemize}
\end{frame}

\begin{frame}{Regression Analysis in R}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item In R, the \texttt{lm()} function is used to fit linear regression models.
        \item The \texttt{lm()} function takes in the formula for the regression model and the data as arguments.
        \item The formula specifies the dependent variable and the independent variables in the regression model.
    \end{itemize}
\end{frame}

\begin{frame}{Regression Analysis in R}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item The \texttt{summary()} function is used to display the results of the regression analysis.
        \item The \texttt{summary()} function displays the estimated coefficients, the standard errors, the t-values, and the p-values of the regression model.
        \item The \texttt{summary()} function also displays the coefficient of determination and the results of the F-test.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Regression Analysis in R}
    \begin{lstlisting}[language=R]
# Generate some data
set.seed(123)
data <- data.frame(
    y = rnorm(100, mean = 5, sd = 2),
    x1 = rnorm(100, mean = 6, sd = 2),
    x2 = rnorm(100, mean = 7, sd = 2)
)

# Fit linear regression model
model <- lm(y ~ x1 + x2, data = data)

# Display results
summary(model)
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Breakdown of Regression Analysis in R}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item \textbf{Set Seed:}
              \begin{itemize}
                  \item \texttt{set.seed(123)}: Sets the seed for random number generation to ensure reproducibility of results.
              \end{itemize}

        \item \textbf{Generate Data:}
              \begin{itemize}
                  \item \texttt{data <- data.frame(y = rnorm(100, mean = 5, sd = 2), x1 = rnorm(100, mean = 6, sd = 2), x2 = rnorm(100, mean = 7, sd = 2)}: Generates 100 random numbers from a normal distribution with specified means and standard deviations, and assigns them to variables \texttt{y}, \texttt{x1}, and \texttt{x2}.
              \end{itemize}

        \item \textbf{Fit Linear Regression Model:}
              \begin{itemize}
                  \item \texttt{model <- lm(y ~ x1 + x2, data = data)}: Fits a linear regression model to predict variable \texttt{y} using variables \texttt{x1} and \texttt{x2} as predictors.
              \end{itemize}

        \item \textbf{Display Results:}
              \begin{itemize}
                  \item \texttt{summary(model)}: Displays the results of the linear regression analysis, including the estimated coefficients, standard errors, t-values, p-values, coefficient of determination, and F-test results.
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Best Practices for Displaying Results}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item Use tables to display the results of the regression analysis.
        \item Include the estimated coefficients, standard errors, t-values, and p-values in the table.
        \item Highlight the statistically significant coefficients in the table.
        \item Include the coefficient of determination and the results of the F-test in the table.
        \item Use visualizations, such as scatter plots and residual plots, to display the relationship between the dependent variable and the independent variables.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Best Practices for Interpreting Results}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item Interpret the estimated coefficients of the independent variables in the regression model.
        \item Interpret the coefficient of determination, which is the proportion of the variance in the dependent variable that is explained by the independent variables.
        \item Interpret the results of the F-test, which tests whether the regression model is a better fit than a model with no independent variables.
        \item Interpret the p-values of the estimated coefficients, which indicate the statistical significance of the coefficients.
    \end{itemize}
\end{frame}

% \section{Time Series Analysis}

% \begin{frame}
%     \frametitle{Table of Contents}
%     \setbeamertemplate{section in toc}[sections numbered]
%     \tableofcontents[currentsection]
% \end{frame}

% \begin{frame}{Time Series Analysis}
%     \begin{itemize}
%         \item Time series analysis is a statistical method that is used to model and forecast time series data.
%         \item Time series data is a sequence of observations that are recorded at regular intervals over time.
%         \item Time series analysis is used in many fields, such as economics, finance, and engineering, to analyze and predict trends and patterns in the data.
%     \end{itemize}
% \end{frame}

% \begin{frame}{Time Series Analysis}
%     \begin{itemize}
%         \setlength{\itemsep}{2em}
%         \item Time series analysis is used to model the underlying structure of the time series data and to make forecasts of future values.
%         \item The goal of time series analysis is to identify the patterns and trends in the data and to make predictions based on these patterns and trends.
%         \item Time series analysis is used to analyze and forecast time series data using statistical models, such as autoregressive integrated moving average (ARIMA) models and exponential smoothing models.
%     \end{itemize}
% \end{frame}

% \begin{frame}{Time Series Analysis}
%     \begin{itemize}
%         \setlength{\itemsep}{2em}
%         \item ARIMA models are a class of models that are used to model time series data that exhibit autocorrelation and seasonality.
%         \item ARIMA models are specified by three parameters: the autoregressive order (p), the differencing order (d), and the moving average order (q).
%         \item Exponential smoothing models are a class of models that are used to model time series data that exhibit trend and seasonality.
%     \end{itemize}
% \end{frame}

% \begin{frame}{Time Series Analysis}
%     \begin{itemize}
%         \setlength{\itemsep}{2em}
%         \item Exponential smoothing models are specified by three parameters: the level parameter (alpha), the trend parameter (beta), and the seasonality parameter (gamma).
%         \item The parameters of the time series models are estimated using the method of maximum likelihood, which maximizes the likelihood of the observed data given the model.
%         \item The goodness of fit of the time series model is measured using the mean squared error (MSE) and the Akaike information criterion (AIC).
%     \end{itemize}
% \end{frame}

% \begin{frame}{Time Series Analysis in R}
%     \begin{itemize}
%         \setlength{\itemsep}{2em}
%         \item In R, the \texttt{arima()} function is used to fit ARIMA models to time series data.
%         \item The \texttt{arima()} function takes in the time series data and the parameters of the ARIMA model as arguments.
%         \item The parameters of the ARIMA model are specified by the order argument, which is a vector of the autoregressive order, the differencing order, and the moving average order.
%     \end{itemize}
% \end{frame}

% \begin{frame}{Time Series Analysis in R}
%     \begin{itemize}
%         \setlength{\itemsep}{2em}
%         \item In R, the \texttt{ets()} function is used to fit exponential smoothing models to time series data.
%         \item The \texttt{ets()} function takes in the time series data and the parameters of the exponential smoothing model as arguments.
%         \item The parameters of the exponential smoothing model are specified by the model argument, which is a string that specifies the type of exponential smoothing model to fit.
%     \end{itemize}
% \end{frame}

% \begin{frame}[fragile]{Time Series Analysis in R}
%     \begin{lstlisting}[language=R]
% # Load packages
% library(forecast)

% # Generate time series data
% data <- ts(data, start = 1, end = n, frequency = f)

% # Fit ARIMA model
% model <- arima(data, order = c(p, d, q))

% # Fit exponential smoothing model
% model <- ets(data, model = "ZZZ")
%     \end{lstlisting}
% \end{frame}

\section{Research Design}

\begin{frame}
    \frametitle{Table of Contents}
    \setbeamertemplate{section in toc}[sections numbered]
    \tableofcontents[currentsection]
\end{frame}

\begin{frame}
    \frametitle{IV in a Nutshell}
    \begin{align}
        y = \beta_0 + \beta_1 x + \epsilon
    \end{align}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item \(x_1\) is not randomly assigned in general (\(\text{cov}(x_1, \epsilon) \neq 0\)), but \dots
        \item Some small part of the variation in \(x_1\) is idiosyncratic
        \item If you could identify and isolate that idiosyncratic variance, you could use it to estimate the causal effect of \(x_1\) on \(y\)
        \item This is the basic idea behind instrumental variables
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Two Stage Least Squares}
    \begin{align}
        x_1 & = \pi_1 z + \pi_2 X_2 + \nu                  \\
        y   & = \beta_1 \hat{x}_1 + \beta_2 X_2 + \epsilon
    \end{align}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item Regress \(x_1\) on the instrument \(z\) to get \(\hat{x}_1\), be sure to add any additional controls in the regression.
        \item Use the predicted values of \(x_1\) in the second stage regression along with any other controls.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{What Makes a Good Instrument}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item Exogeneity: cov(\(z, \epsilon\)) = 0
              \begin{itemize}
                  \item The instrument is uncorrelated with the error term
                  \item You can test this using observables
                  \item But this is necessary and not sufficient, you would have to argue this. There may be unobservables that are correlated with the instrument and the error term.
              \end{itemize}
        \item Excludability: The instrument is not correlated with the dependent variable ( The only way the instrument affects the dependent variable is through the independent variable. )
        \item Relevance: The instrument is correlated with the independent variable ( This is testable )
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Local Average Treatment Effect}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item Homogenous Treatment Effects
              \begin{itemize}
                  \item This means that the treatment effect is the same for everyone
              \end{itemize}
        \item If we assume that the treatment effect is homogenous for everyone, then the instrumental variable estimates the local average treatment effect (LATE)
        \item This means the average treatment effect for the subpopulation of individuals who are affected by the instrument
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Subgroups}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item Always takers: People who would take the treatment regardless of the instrument
        \item Never takers: People who would never take the treatment regardless of the instrument
        \item Compliers: People who would take the treatment if the instrument is high or low enough
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{LATE Assumptions}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item Independence: The instrument is as good as randomly assigned
        \item Exclusion restriction: The instrument only affects the dependent variable through the independent variable
        \item Monotonicity: There are no defiers (non compliers)
              \begin{itemize}
                  \item Either $\pi_{1i} \geq 0 \forall i$ or $\pi_{1i} \leq 0 \forall i$
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Instrumental Variables in R}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item The \texttt{ivreg()} function estimates the parameters of the regression model using the method of instrumental variables.
        \item The \texttt{ivreg()} function returns the estimated coefficients, the standard errors, the t-values, and the p-values of the regression model.
        \item The \texttt{ivreg()} function is used to estimate the causal effect of the independent variable on the dependent variable by removing the correlation between the independent variable and the error term.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Instrumental Variables in R}
    \begin{lstlisting}[language=R]
library(AER)
# Generate some data 
set.seed(123)
n <- 100
x <- rnorm(n)
z <- rnorm(n)
y <- 1 + 2 * x + 3 * z + rnorm(n)

# Fit instrumental variables regression model
model <- ivreg(y ~ x | z)
summary(model)
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Breakdown of Instrumental Variables in R}
    \begin{itemize}
        \setlength{\itemsep}{2em}
        \item \textbf{Load Package:}
              \begin{itemize}
                  \item \texttt{library(AER)}: Loads the AER package, which contains the \texttt{ivreg()} function for estimating instrumental variables regression models.
              \end{itemize}

        \item \textbf{Generate Data:}
              \begin{itemize}
                  \item \texttt{x <- rnorm(n)}: Generates \texttt{n} random numbers from a normal distribution and assigns them to vector \texttt{x}.
                  \item \texttt{z <- rnorm(n)}: Generates \texttt{n} random numbers from a normal distribution and assigns them to vector \texttt{z}.
                  \item \texttt{y <- 1 + 2 * x + 3 * z + rnorm(n)}: Generates \texttt{n} random numbers from a normal distribution and assigns them to vector \texttt{y}.
              \end{itemize}

        \item \textbf{Fit Instrumental Variables Regression Model:}
              \begin{itemize}
                  \item \texttt{model <- ivreg(y ~ x | z)}: Fits an instrumental variables regression model to predict variable \texttt{y} using variable \texttt{x} as a predictor and variable \texttt{z} as an instrumental variable.
              \end{itemize}

        \item \textbf{Display Results:}
              \begin{itemize}
                  \item \texttt{summary(model)}: Displays the results of the instrumental variables regression analysis, including the estimated coefficients, standard errors, t-values, and p-values.
              \end{itemize}
    \end{itemize}
\end{frame}

\end{document}