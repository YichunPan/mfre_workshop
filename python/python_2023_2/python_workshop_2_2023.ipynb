{"cells":[{"cell_type":"markdown","metadata":{"id":"emQ8JsDNhle6"},"source":["# MFRE Summer Session: Python Workshop 2\n","\n","\n","## 1.\tData Manipulation with `pandas`\n","- 1.1\tIntroduction to Pandas Library for data manipulation\n","- 1.2\tWorking with pandasâ€™ data structures (Series, data frame)\n","\n","\n","\n","\n","## 2. Hands-on Work: Data Cleaning and Exploratory Analysis\n","\n","2.0  Data import\n","- Code taught: `pd.read_csv()`\n","\n","2.1\tHow many rows and columns are in the sector GHG emissions dataset?\n","- Code taught: `.shape`\n","- Exercise: how many in regional GHG emissions dataset? Larger/smaller than sector?\n","\n","2.2\tWhat are the column names and data types in the dataset?\n","- Code taught: `.columns`, `.dtypes`\n","- Exercise: Get the column names and datatypes of the `regional_emissions` dataset.\n","\n","2.3\tWhat is the total greenhouse gas emissions for each province in Canada?\n","- Code taught: `.sum()`\n","- Exercise: get sum of each column and row of `regional_emissions`. How are these values different?\n","\n","2.4\tWhich industry had the single highest year of emissions? The single lowest? What were these values?\n","- Code taught: `.idxmax()`, `.idxmin()`, `.max()`, `.min()`\n","- Exercise: get these from `regional_emissions`\n","\n","2.5 Slicing and selection with `.loc[]`, `.iloc[]`\n","- Code taught: `.loc[]`, `.iloc[]`\n","\n","2.6 Filtering with boolean conditions\n","- Code taught: `df[df[\"column\"] < value]`\n","\n","2.7 Column Operations\n","- Code taught: `df[\"new_column\"] = df[\"column_a\"]*df[\"column_b\"]`\n","- calculating percentage change over time\n","\n","2.8 Summary Stats: pandas function\n","- `.describe()`\n","\n","2.9 Open-Ended Exercise"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1S1Ib2HhlfB"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"txV-JRHUhlfD"},"source":["#  0. Data Import with `read_csv()`"]},{"cell_type":"markdown","metadata":{"id":"WYm9N0QdhlfE"},"source":["![memorize all of these](https://i.imgur.com/BJq4hmO.png)"]},{"cell_type":"markdown","metadata":{"id":"vmmwrR50hlfF"},"source":["Above is the Pandas `read_csv()` function, accessed as above, with `pd.read_csv()`.\n","\n","A partial list of the function's possible arguments is shown in the image; you will need to memorize all of these, as well as some others that aren't printed, if you want to succeed in the MFRE program."]},{"cell_type":"markdown","metadata":{"id":"FWso_MF7hlfG"},"source":["I will close this page and we'll go around the room. Everyone needs to give a full explanation of any one of these arguments."]},{"cell_type":"markdown","metadata":{"id":"HwcPsIQQhlfG"},"source":["..."]},{"cell_type":"markdown","metadata":{"id":"oW4LXTxWhlfH"},"source":["...\n"]},{"cell_type":"markdown","metadata":{"id":"_MGnfchrhlfI"},"source":["..."]},{"cell_type":"markdown","metadata":{"id":"ZUnNW4Q-hlfI"},"source":["..."]},{"cell_type":"markdown","metadata":{"id":"0YRsATtehlfI"},"source":["..."]},{"cell_type":"markdown","metadata":{"id":"bVNTVmKwhlfJ"},"source":["Yeah, I'm kidding. I've never read most of these, let alone having used them.\n","\n","For the normal case, where you're using Python based in a development environment on your own computer, you'd follow the following process:\n","\n","Call `pd.read_csv()` with just one argument: the path to your file. I'll go over the typical process I use for this.\n","\n","![image1](https://i.imgur.com/PQ22zSg.png)\n","\n","First, right-click your `.csv` file, and click *Properties* in the menu, circled in red.\n","\n","![image2](https://i.imgur.com/hiGKFcc.png)\n","\n","Next, navigate to the *Security* tab circled in red, and copy the `Object name` section, circled in blue.\n","\n","Next, go into your Python workbook, and paste this into the `pd.read_csv()` file, surrounded by quotation marks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xVTPaJo-hlfJ"},"outputs":[],"source":["# this won't work\n","\n","regional_emissions = pd.read_csv(\"C:\\Users\\jizatt\\Documents\\summer_session_workshops\\regional_emissions.csv\")"]},{"cell_type":"markdown","metadata":{"id":"qCBX4czvhlfK"},"source":["But this throws an error!\n","\n","Frustratingly, the function is set to need all \"/\" slashes, not \"\\\" slashes as default.\n","\n","Don't ask me if these are back-slashes or forward slashes - I never could remember!\n","\n","We have to go back through and replace all of these."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cVpqhBOhhlfK"},"outputs":[],"source":["# replace all \\ slashes with / slashes and this should work!\n","\n","regional_emissions = pd.read_csv(\"C:/Users/jizatt/Documents/summer_session_workshops/MFRE-summer-session-workshops-2023/regional_emissions.csv\")\n","\n","regional_emissions.head()\n","\n","# this won't work either"]},{"cell_type":"markdown","metadata":{"id":"HKwyEwGMhlfK"},"source":["If we were working in Anaconda, or VSCode, this would work fine, since those are based out of your own computer. So, remember this process for if you do work in those environments!\n","\n","We're using Google Colaboratory, which based in the cloud. This means it doesn't have inherent access to your system files.\n","\n","The overall easiest way to import data, regardless of your platform, involves using Google Drive. I'll use it to *actually* import the workshop data.\n","\n","First, you upload your `.csv` data file to Google Drive, and set sharing to \"Anyone with the link\", then copy the download link. You will use the following code to adjust the URL for download."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDQ-5ue-hlfL"},"outputs":[],"source":["# regional_emissions.csv Google Drive method\n","\n","url = \"https://drive.google.com/file/d/1M0Ab8MwvP9d7_Lr-p09xYeJqVsJzB-x4/view?usp=sharing\"\n","path = \"https://drive.google.com/uc?export=download&id=\"+url.split('/')[-2]\n","regional_emissions = pd.read_csv(path)\n","\n","regional_emissions.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lc84dT__hlfL"},"outputs":[],"source":["# sector_emissions.csv Google Drive method\n","\n","url = \"https://drive.google.com/file/d/1Mq5fuv5tqBexcjbuuZhoT2m-IpCb8SSX/view?usp=sharing\"\n","path = \"https://drive.google.com/uc?export=download&id=\"+url.split('/')[-2]\n","sector_emissions = pd.read_csv(path)\n","\n","sector_emissions.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fO2jzhxVhlfM"},"outputs":[],"source":["# in case google drive method doesn't work, we can import manually with this as base\n","\n","# sector_emissions = pd.read_csv(\"C:/Users/jizatt/Documents/summer_session_workshops/sector_emissions.csv\")"]},{"cell_type":"markdown","metadata":{"id":"t63gHJX0hlfM"},"source":["# 1.\tHow many rows and columns are in the sector emissions dataset?"]},{"cell_type":"markdown","metadata":{"id":"ZU7TYZZYhlfM"},"source":["When you're working with a Pandas DataFrame object, it's important to know the shape; this is a `(n, q)` tuple where `n` is the length and `q` is the width.\n","\n","Typically, a dataset's rows are samples while its columns are variables, so `n` tells you the sample size, and `q` tells you the number of variables counted."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5zaTkkt-hlfM"},"outputs":[],"source":["sector_emissions.shape"]},{"cell_type":"markdown","metadata":{"id":"h4VX4e31hlfM"},"source":["Length comes first, then Width. The former is how many rows, i.e. samples the dataframe has; the latter is how many columns, i.e. variables.\n","\n","What if we look at just one column?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ycia7m6JhlfN"},"outputs":[],"source":["sector_emissions[\"Agriculture\"].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Wc4tBaZhlfN"},"outputs":[],"source":["type(sector_emissions[\"Agriculture\"])"]},{"cell_type":"markdown","metadata":{"id":"vS0TECfChlfN"},"source":["When we call just one column of a DataFrame, it turns into a series, which gets reported as `(32,)`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jkflbpn7hlfN"},"outputs":[],"source":["sector_emissions[[\"Agriculture\", \"Heavy industry\"]].shape"]},{"cell_type":"markdown","metadata":{"id":"PAAxYRKBhlfN"},"source":["However, when we call its columns with a list of column names - even just one - it stays as a DataFrame and gets reported as `(32, 1)`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"waoNhRy7hlfN"},"outputs":[],"source":["sector_emissions[[\"Agriculture\"]].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ODrgGGSjhlfN"},"outputs":[],"source":["type(sector_emissions[[\"Agriculture\"]])"]},{"cell_type":"markdown","metadata":{"id":"lJWpXcNThlfO"},"source":["# Exercise 1:\n","\n","Get the shape of `regional_emissions`. How many rows and columns are in it?\n","\n","Is it a bigger or smaller dataset than `sector_emissions`?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0CUBCun1hlfO"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7VQNTCePhlfO"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Bz5bNXhBhlfO"},"source":["We can see it's smaller in both length (13 vs 30 observations) and width (5 vs 8 observations), so it must be smaller overall."]},{"cell_type":"markdown","metadata":{"id":"EJ-D3rx3hlfO"},"source":["# 2.\tWhat are the column names and data types in the dataset?\n"]},{"cell_type":"markdown","metadata":{"id":"_tgq6tvAhlfO"},"source":["While we can just call a DataFrame and read off the columns, there will be times you want to specifically get the list of column names in a dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y1oBCDU9hlfO"},"outputs":[],"source":["sector_emissions.columns"]},{"cell_type":"markdown","metadata":{"id":"6SXOVC3BhlfO"},"source":["You can use `df_name.columns` and have Python return them that way. However, this can't be taken and inputted elsewhere directly.\n","\n","What about `list(df_name.columns)` ?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_cTlva5IhlfO"},"outputs":[],"source":["list(sector_emissions.columns)"]},{"cell_type":"markdown","metadata":{"id":"iGtnz0I9hlfT"},"source":["That's more like it. We can slice this just like any other list:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9rr5q0rChlfT"},"outputs":[],"source":["list(sector_emissions.columns)[:4] # first four columns"]},{"cell_type":"markdown","metadata":{"id":"dpmHQfGjhlfT"},"source":["To get the data type of each column, use `df_name.dtypes`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CWakltjKhlfT"},"outputs":[],"source":["sector_emissions.dtypes"]},{"cell_type":"markdown","metadata":{"id":"AhDR1-MAhlfU"},"source":["### Why bother? Reasoning behind Parts 1/2\n","\n","When you're importing a dataset, you probably have some idea of what size it is - roughly how many variables it has, and what ballpark the sample size is in. Calling `.shape` lets you quickly check if something is glaringly wrong. `.colnames` and `.dtypes` let you make sure that everything you need is there, and that all the data are in the format you expect.\n","\n","This way, if something is wrong, you can catch it early, before you spend a bunch of time stumbling around through various error messages, or worse, doing work without knowing something is wrong!"]},{"cell_type":"markdown","metadata":{"id":"Sfb7V6TShlfU"},"source":["# Exercise 2:\n","\n","Get the column names and datatypes of the `regional_emissions` dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c5aDhGr9hlfU"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RL8BHCAVhlfU"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Vrlo8PM7hlfU"},"source":["# 3. What is the total greenhouse gas emission for each economic sector in Canada?"]},{"cell_type":"markdown","metadata":{"id":"RA-2dgzkhlfU"},"source":["If you remember from Workshop 1, we actually did this there too. It involved taking every data column of `sector_emissions` and looping through it, adding up each value until we had our total.\n","\n","There are two issues with this approach.\n","\n","1. It's code-intensive. We had to write each loop, or at least copy and modify the loop, for each column. This creates work for us, and fills a lot of space in our workbook, making the overall file less easily readable.\n","\n","2. It's processing-inefficient. This dataset is small, so that's fine. But if you work with datasets of thousands, tens of thousands, hundreds of thousands, or more samples, then looping will rapidly become infeasible.\n","\n","I once asked a professor for help with code for my undergraduate thesis project. When he saw I was using loops to clean my dataset, he couldn't stop laughing. He was right, and still a big help! But let me spare you some pain.\n","\n","(And I promise not to laugh at you!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3JuO5dlOhlfV"},"outputs":[],"source":["sector_emissions.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4d72wNnYhlfV"},"outputs":[],"source":["# Python Workshop 1-style approach\n","\n","oil_gas = sector_emissions[\"Oil and gas\"]\n","oil_gas_total = 0\n","\n","for num in oil_gas:\n","    oil_gas_total += num\n","\n","oil_gas_total"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VNxHra8ahlfV"},"outputs":[],"source":["list(sector_emissions.columns)[1:]"]},{"cell_type":"markdown","metadata":{"id":"SyozznVAhlfV"},"source":["We can actually run this as a loop within a loop. For each column, we'll total up the values within and print them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"knw5axCnhlfV"},"outputs":[],"source":["%%time\n","\n","for column in list(sector_emissions.columns)[1:]:\n","    total = 0\n","    for num in sector_emissions[column]:\n","        total += num\n","    print(column + \" total equals: \" + str(total))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KJsB-ummhlfV"},"outputs":[],"source":["%%time\n","\n","sector_emissions[1:].sum()"]},{"cell_type":"markdown","metadata":{"id":"TtVZKHCuhlfW"},"source":["The CPU time is close to zero, and the Wall time is also insignificant, but even this reveals how the vectorized method with`.sum()` takes around half the time of the list.\n","\n","The distinction becomes much more clear when the sample size gets bigger:\n","\n","![stack_overflow](https://i.imgur.com/Zk7op8v.png)\n","\n","[Source](https://stackoverflow.com/questions/54028199/are-for-loops-in-pandas-really-bad-when-should-i-care)\n","\n","List comprehensions are optimized versions of loops, written in a different format, but the point should hold; out beyond a few thousand sample size, they are slower and scale linearly with `n` value. In comparison, vectorized operations such as with Pandas functions (built out of Numpy) rise in computation time only slowly.\n","\n","The scale is worth noting here; at any sample size you are likely to work on in MFRE, all of these will be negligible. But with large data the difference does grow, and other concerns such as readability still push us towards using functions over loops."]},{"cell_type":"markdown","metadata":{"id":"JgnWNRX_hlfW"},"source":["# Exercise 3:\n","\n","Take the `regional_emissions` dataset and find the sum of each column. What have you calculated with this?\n","\n","Next, input the argument `axis = 1` and take the `.sum()`. What did you calculate this time?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"schl7hpbhlfW"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fJwVPdmChlfW"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FanlRPDfhlfX"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"GsiswXArhlfX"},"source":["However, you should still stick with dedicated functions over loops when possible. They're much more readable, and easy to write as well. Compare the 5 lines of code for the loop above with the 1 line for the `.sum()` call."]},{"cell_type":"markdown","metadata":{"id":"GZwux8HihlfX"},"source":["# 4. Which industry had the single highest year of emissions? The single lowest?\n","\n","The index of a Pandas DataFrame is the set of labels running down the left-hand side; for `sector_emissions`, this means the 0, 1, 2, 3, etc, up to 31 in the bottom row. The index is vital for accessing, aligning, and joining datasets.\n","\n","For example, if you have two different yearly datasets, one with values of interest rates, and one with unemployment rates, you could use the yearly index to join both together so that all the years match.\n","\n","We'll use it here to find certain values, with the `.idxmax()`, `.idxmin()`, `.max()`, and `.min()` functions.\n","\n","But first, we'll need to explicitly set our index as the `\"Year\"` column, with `.set_index()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fm7W9fuFhlfX"},"outputs":[],"source":["sector_emissions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"km1gXHz3hlfX"},"outputs":[],"source":["sector_emissions_index = sector_emissions.set_index(\"Year\")\n","sector_emissions_index"]},{"cell_type":"markdown","metadata":{"id":"RKVgdXJYhlfX"},"source":["As you saw, we assigned this as a new variable, `sector_emissions_index`, naming the `\"Year\"` column as the index.\n","\n","You can also just set the index on the current dataframe. By default, `set_index()` just creates the new variable and returns it; you have to assign it to a new variable like we did above.\n","\n","Instead, we can use the argument `inplace = 1`. This of this as saying \"set the index to year, and perform it in place.\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8TkXmTWAhlfY"},"outputs":[],"source":["sector_emissions.set_index(\"Year\", inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qrIZ01LihlfY"},"outputs":[],"source":["sector_emissions.head()"]},{"cell_type":"markdown","metadata":{"id":"fbgvnsxahlfY"},"source":["We want one of these in \"vanilla\" form, so I'll use `.reset_index()` to turn it back to normal."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QU5gjzeghlfY"},"outputs":[],"source":["sector_emissions.reset_index(inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iutoyOIvhlfY"},"outputs":[],"source":["sector_emissions.head()"]},{"cell_type":"markdown","metadata":{"id":"88Ar5FW2hlfY"},"source":["And as you can see, the index is gone.\n","\n","#### Why does the index matter?\n","\n","1. It labels and identifies the rows.\n","2. We can select and slice data using it.\n","3. We can join datasets using it.\n","\n","Numbers 1 and 2 are our main interests here; Joining is a topic for another day. But Even just for what we're doing now, the index plays a vital role.\n","\n","\n","# Exercise 4a:\n","\n","Take the `regional_emissions` index. Using the `.set_index()` command, create a variable called `regional_emissions_index` with the `\"Region\"` variable set as its index.\n","\n","Then, set the index of `regional_emissions` as `\"Region\"` without creating a new variable.\n","\n","Last, reset the index of `regional_emissions`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gQbvp20bhlfZ"},"outputs":[],"source":["regional_emissions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DWH2R2hxhlfZ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Em6l2QyBhlfZ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JaWzef1DhlfZ"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"LCjph7zWhlfZ"},"source":["### [Exercise End]"]},{"cell_type":"markdown","metadata":{"id":"H6g_KDpihlfZ"},"source":["# `.max()`, `.min()`, `.idxmax()`, `.idxmin()`\n","\n","Now that we've seen how the index works, we'll use it with the above functions to retrieve the greatest and lowest values of the columns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MbHYdfA6hlfa"},"outputs":[],"source":["sector_emissions_index.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mO69M6_Mhlfa"},"outputs":[],"source":["sector_emissions_index.min()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aTX8fCbBhlfa"},"outputs":[],"source":["sector_emissions_index"]},{"cell_type":"markdown","metadata":{"id":"WN4WbkrAhlfa"},"source":["As we can see, using the `.max()` and `.min()` methods on the indexed datasets directs Python to return the greatest or lowest value in each column.\n","\n","We may also be interested in what *year* we have these maximums and minimums. When our index is the `\"Year\"` variable, the `.idxmax()` and `.idxmin()` values can return the matching years:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JUTS1Fu5hlfa"},"outputs":[],"source":["sector_emissions_index.idxmax()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCAb8WTghlfb"},"outputs":[],"source":["sector_emissions_index.idxmin()"]},{"cell_type":"markdown","metadata":{"id":"fLOKOaUKhlfb"},"source":["There's some interesting information here. Most of the sectors had their maximum emissions levels recently, and their lowest near the start of the dataset. But `\"Electricity\"` maxed out in 2001 and had its minimum value in the most recent year, 2021. `\"Heavy industry\"` and `\"Waste and others\"` also have unusual patterns."]},{"cell_type":"markdown","metadata":{"id":"IiA9YrwKhlfb"},"source":["# Exercise 4b:\n","\n","Take the `.max()`, `idxmax()`, `.min()`, and `.idxmin()` values of the `regional_emissions_index` dataset. Do you notice any interesting patterns?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1hPkPmCohlfb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NKRYZfZ2hlfc"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-c1tC347hlfc"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"gBirCZV9hlfc"},"source":["# 5. Slicing and Selection with `.loc[]` and `.iloc[]`\n","\n","When we were working with lists, we often would \"slice\" out specific sections by putting a number, or series of numbers, in behind a list object.\n","\n","For example:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3RvqXKTLhlfd"},"outputs":[],"source":["simple_list = [1, 2, 3, 4, 5]\n","\n","simple_list[2:4]"]},{"cell_type":"markdown","metadata":{"id":"x6B_rtZvhlfd"},"source":["Filtering a DataFrame involves reducing the full dataframe to a subset, where the set of rows match one or more conditions. The `.loc[]` and`.iloc[]` functions can both do this, but approach it in a different way:\n","\n","- `.loc[]` is based on column/row labels, or names. You name your columns and what condition must apply to them. The dataset returned will be limited to rows where your conditions are met.\n","\n","- `.iloc[]` does the same, but specifying rows/columns by their integer places.\n","\n","\n","I'd advice you to prioritize `.loc[]`, because it isn't sensitive to columns having their ordering changed."]},{"cell_type":"markdown","metadata":{"id":"dRudfPILhlfd"},"source":["### Data Selection: One Value\n","\n","Note: `df` is a generic name for any Pandas DataFrame object.\n","\n","- `df.loc[row_label, column_label]`\n","\n","- `df.iloc[row_position, column_position]`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QWRKAF6jhlfe"},"outputs":[],"source":["regional_emissions.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGfLo3TDhlfe"},"outputs":[],"source":["# this retrieves the value in row position 2, column position 3: Nova Scotia's 2005 emission value\n","regional_emissions.iloc[2,3]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tJmRF9zqhlfe"},"outputs":[],"source":["# this retrieves the value with row label 2, column label \"2005\": Nova Scotia's 2005 emission value\n","\n","regional_emissions.loc[2,\"2005\"]"]},{"cell_type":"markdown","metadata":{"id":"tRQoVJkzhlfe"},"source":["### Data Selection: Slicing\n","\n","We can slice in DataFrames like we slice in lists; you specify a column or row to look into, and a range within it to return.\n","\n","We'll be working with `regional_emissions_index` for this section, so that we can pick out regions by name with `.loc[]` statements."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"llWDC0Zphlfe"},"outputs":[],"source":["regional_emissions_index"]},{"cell_type":"markdown","metadata":{"id":"_JNUO_B-hlfe"},"source":["To get a row:\n","\n","- `.iloc[]`: first input the row's index number, and then the index numbers of the columns you want\n","\n","- `.loc[]`: first input the row's index label, and then the index labels of the columns you want\n","\n","You can select just one row or column, in which case you just input the number/name. Or, you can input a slice statement for the ones you want (around a `:`), or a list, or just a `:` to return everything.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bvhyzgRUhlfe"},"source":["### 1. `.iloc[]` examples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgDRskSrhlff"},"outputs":[],"source":["# one specific value\n","\n","regional_emissions_index.iloc[1, 1] # one value\n"]},{"cell_type":"markdown","metadata":{"id":"XxdJlrjIhlff"},"source":["### 1.1 One row, selecting columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CfMOweg2hlff"},"outputs":[],"source":["regional_emissions_index.iloc[1, 1:4] # a slice from a row"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XVBHPe1bhlff"},"outputs":[],"source":["regional_emissions_index.iloc[1, [1,2,3]] # a list from a row (equivalent to the slice!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_EI0ILEahlff"},"outputs":[],"source":["regional_emissions_index.iloc[1, :] # a whole row"]},{"cell_type":"markdown","metadata":{"id":"7a6wwdVghlff"},"source":["### 1.2 One column, selecting rows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73sdHHr_hlff"},"outputs":[],"source":["regional_emissions_index.iloc[:, 1] # a column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"06tZ4B_qhlff"},"outputs":[],"source":["regional_emissions_index.iloc[1:4, 1] # a slice of rows in one column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xM_xIEXHhlfg"},"outputs":[],"source":["regional_emissions_index.iloc[[1,2,3], 1] # a list of rows in one column (equivalent to the slice!)"]},{"cell_type":"markdown","metadata":{"id":"jp4NV29Ghlfg"},"source":["### 1.3 Multiple rows, multiple columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7MvjSlR7hlfg"},"outputs":[],"source":["regional_emissions_index.iloc[1:4, 2:4] # a set of rows and columns together (with slice)"]},{"cell_type":"markdown","metadata":{"id":"b0L9_TUphlfg"},"source":["### 2. `.loc[]` examples"]},{"cell_type":"markdown","metadata":{"id":"s4mCfx9Whlfg"},"source":["### 2.1 One value"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wW900CY4hlfg"},"outputs":[],"source":["regional_emissions_index.loc[\"Nova Scotia\", \"1990\"] # one value\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2W046Id1hlfg"},"source":["### 2.2 One row, selecting columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O1A9P0Mthlfh"},"outputs":[],"source":["regional_emissions_index.loc[\"Nova Scotia\", \"1990\":\"2021\"] # a slice from a row\n","regional_emissions_index.loc[\"Nova Scotia\", [\"1990\", \"2005\", \"2021\"]] # a list from a row (equivalent to the slice!)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QA-oMRqDhlfh"},"outputs":[],"source":["regional_emissions_index.loc[\"Nova Scotia\", [\"1990\", \"2005\", \"2021\"]] # a list from a row (equivalent to the slice!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wqp501TQhlfh"},"outputs":[],"source":["regional_emissions_index.loc[\"Nova Scotia\", :] # a whole row"]},{"cell_type":"markdown","metadata":{"id":"1eiCW5Cphlfh"},"source":["### 2.3 One column, selecting rows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WRKL0XJJhlfh"},"outputs":[],"source":["regional_emissions_index.loc[:, \"1990\"] # a column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uD497Qzuhlfh"},"outputs":[],"source":["regional_emissions_index.loc[\"Prince Edward Island\":\"New Brunswick\", \"1990\"] # a slice of rows in one column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vUCDL-ZVhlfh"},"outputs":[],"source":["regional_emissions_index.loc[[\"Prince Edward Island\", \"Nova Scotia\", \"New Brunswick\"], \"1990\"] # a list of rows in one column (equivalent to the slice!)"]},{"cell_type":"markdown","metadata":{"id":"5yO4cfQUhlfi"},"source":["### 2.4 Multiple columns and rows: with lists\n","\n","When we did this in `.iloc[]`, we used slices. Here we use lists, to show that works too."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"walbWlwbhlfi"},"outputs":[],"source":["regional_emissions_index.loc[[\"Prince Edward Island\", \"Nova Scotia\", \"New Brunswick\"], [\"1990\", \"2005\", \"2021\"]] # a set of rows and columns together (with list)"]},{"cell_type":"markdown","metadata":{"id":"euS_Xu_Rhlfi"},"source":["## `.loc[]` versus `.iloc[]`: Which to use and why?\n","\n","For general selection of columns, I'd recommend you to stick with `.loc[]`. Column names are much more likely to remain stable than data positioning.\n"]},{"cell_type":"markdown","metadata":{"id":"ErGd6ojihlfi"},"source":["#### Say that column names are changed, affecting `.loc[]`, or positions change, affecting `.iloc[]`. Which is worse?\n","\n","In my opinion, the latter is **much worse**.\n","\n","- If column names change, `.loc[]` will throw an error message and break.\n","\n","- But if positions change, `.iloc[]` will grab whatever's there and keep going without batting an eyelid. This could feed in the wrong data at a later point without you having any idea what's going on!"]},{"cell_type":"markdown","metadata":{"id":"TbNj7Ozdhlfi"},"source":["### In Defense of `.iloc[]`: sorting\n","\n","One functionality `.iloc[]` has which `.loc[]` definitely doesn't relates to sorting. If you use another function to sort a DataFrame by its values in one column, you can use `.iloc[]` to grab the top X, or bottom X, rows, getting you the rows with the highest or lowest values of that variable.\n","\n","For example, below we sort 2021 carbon emissions (greatest to least), then take the 5 at the top of the DataFrame. This gives us the 5 provinces with the highest emissions in 2021.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6RGWzLvdhlfi"},"outputs":[],"source":["# get 5 rows with lowest 2021 values\n","\n","regional_emissions.sort_values(\"2021\", ascending = True).iloc[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lsH45r3Vhlfi"},"outputs":[],"source":["# get 5 rows with highest 2021 values\n","\n","regional_emissions.sort_values(\"2021\", ascending = False).iloc[:5]"]},{"cell_type":"markdown","metadata":{"id":"j7POSYeghlfj"},"source":["# Data Selection: Columns\n","\n","Simply grabbing one column, or a set of them, is easier than grabbing specific bits of data.\n","\n","For one column, use the format `df[\"column\"]`. Think of this as telling Python to give you just `\"column\"` from the dataframe called `df`\n","\n","For multiple columns, you pass a list of columns, like `df[\"[column_1\", \"column_2\"]]`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dj7_Way2hlfj"},"outputs":[],"source":["regional_emissions[\"Region\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K7X79VY_hlfj"},"outputs":[],"source":["regional_emissions[[\"1990\", \"2005\", \"2021\"]]"]},{"cell_type":"markdown","metadata":{"id":"9t-3tPM-hlfj"},"source":["# Exercise 5:\n","\n","Create `sector_emissions_2000s`, including only rows of `sector_emisions` with a `Year` of 2000 or greater.\n","\n","Then, access `sector_emissions_2000s` columns one at a time, and `.sum()` each up, recording these as an identifiable variable.\n","\n","Lastly, add up all of these values to get the total emissions in Canada from 2000 onwards."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9cJ2_ii0hlfk"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s6YyiNI8hlfk"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"hJWJaXmuhlfk"},"source":["# 6. Filtering\n","\n","\n","If you're like me, you're probably not paying much attention anymore. The dude up at the front is just going on and on about \"locks\" and \"iLocks\".\n","\n","(Some weird new Apple release?)\n","\n","But this part is important. A **very** common operation in data cleaning is *\"Filtering\"*, where you take a large dataset and reduce it to just the samples that fit certain characteristics.\n","\n","\n","Examples:\n","\n","- If you've got data on crop returns in Canadian provinces, you might filter to just the samples from Alberta and British Columbia.\n","- If you've got macroeconomic data, you might filter to just samples from 2000 onwards.\n","- If you've got a medicinal study, you might filter to just candidates with a certain health condition.\n","\n","## Can anyone come up with an idea like this - a theoretical dataset, and a criteria you want to filter it by?\n"]},{"cell_type":"markdown","metadata":{"id":"s3ITwLaXhlfk"},"source":["### Core idea\n","\n","There are a few ways to filter data. What we'll be doing revolves around creating a \"boolean array\", and then selecting rows with it.\n","\n","A boolean array is created by comparing a logical \"True or False\" condition with some value in each row. For example, in a dataset on daily weather reports, if the High temperature exceeded 20.0C.\n","\n","Then, the rest of the expression subsets the dataset to just the rows where this condition is true. We'll work through some examples below."]},{"cell_type":"markdown","metadata":{"id":"Bso_MFzrhlfk"},"source":["#### 6.1 One condition\n","\n","The generalized syntax for this goes as such:\n","\n","`df[df[\"column\"] <boolean operator> <value>]`\n","\n","- `df` is the DataFrame you're filtering\n","- `\"column\"` is the column whose values you're examining\n","- `<boolean operator>` is an operator, like `<`, `>`, `==`, `<=`, or `>=`\n","- `<value>` is just some value\n","\n","What happens then, is that Python uses the `<boolean operator>` term to compare `<value>` with the row's `\"column\"` value, returning `True` or `False`. Because it does this for each row, it creates our \"boolean array\"\n","\n","Finally, `df` is subsetted to just the rows which line up with a `True` value, while the `False` values disappear.\n","\n","We'll use some examples to get a feel for this."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DdHoxdDShlfl"},"outputs":[],"source":["regional_emissions[regional_emissions[\"Shortnam\"] == \"NL\"]"]},{"cell_type":"markdown","metadata":{"id":"xxelje43hlfl"},"source":["What's going on here?\n","\n","As you can see, we're comparing the `\"Shortnam\"` column with a string value."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"he9-PBkRhlfl"},"outputs":[],"source":["regional_emissions[\"Shortnam\"]"]},{"cell_type":"markdown","metadata":{"id":"nX7Cx8c7hlfl"},"source":["Because only one of these is equal to `\"NL\"`, we filter down to just that row: Newfoundland and Labrador."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zV0mD1D3hlfl"},"outputs":[],"source":["regional_emissions[regional_emissions[\"1990\"] <= 10.0]"]},{"cell_type":"markdown","metadata":{"id":"fTY_ygt9hlfm"},"source":["Our next example uses a numeric comparison: it makes a boolean array based on if each row's `\"1990\"` value is equal or less than 10.0, and returns these."]},{"cell_type":"markdown","metadata":{"id":"wXG5_mUehlfm"},"source":["#### Multiple Conditions\n","\n","Inside of your `df[]` statement, you can include multiple conditions. These can be joined with either the `&` operator (meaning both must be true for a True entry in the boolean array) or an `|` operator (meaning either being true is sufficient).\n","\n","\n","It's best to wrap the boolean statements inside of round brackets `()` in order to keep them clearly separated and readable; a line break in between is also good practice.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jo1dHQihhlfm"},"outputs":[],"source":["regional_emissions[(regional_emissions[\"1990\"] < 10) and\n","                   (regional_emissions[\"2005\"] > 10.0)]"]},{"cell_type":"markdown","metadata":{"id":"36hYz-7Whlfm"},"source":["As you can see, the normal `and` operator throws an error."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"amLJNdnlhlfm"},"outputs":[],"source":["regional_emissions[(regional_emissions[\"1990\"] < 10) &\n","                   (regional_emissions[\"2005\"] > 10.0)]"]},{"cell_type":"markdown","metadata":{"id":"SAlZ1yfVhlfn"},"source":["You could also just use two filters in order to get the same result."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WqNDCdM3hlfn"},"outputs":[],"source":["regional_emissions_1990_under_10 = regional_emissions[regional_emissions[\"1990\"] < 10]\n","regional_emissions_1990_under_10_and_2005_above_10 = regional_emissions_1990_under_10[regional_emissions_1990_under_10[\"2005\"] > 10.0]\n","\n","regional_emissions_1990_under_10_and_2005_above_10"]},{"cell_type":"markdown","metadata":{"id":"RXzSdVtBhlfn"},"source":["# Legibility - Writability Tradeoff\n","\n","As you can see, this gets us to the same place. It also illustrates one issue in data cleaning; naming the versions of your dataset.\n","\n","`regional_emissions_1990_under_10_and_2005_above_10` is descriptive, but it's not convenient to write.\n","\n","Or say, for that matter.\n","\n","But a shorter name might lose out on important information for keeping track of what's what. `regional_emissions_filtered` is a lot quicker to write; but what filtering did you do?\n","\n","Unless you're only doing one filter operation (and you never do just one!) you'll very quickly get confused. I don't have a perfect guide for this, just suggestions:\n","\n","1. Start your dataframe names small. `reg_df` standing for \"regional emissions dataframe\" might have been the better choice.\n","2. If you modify a dataset and don't need to refer to pre-change versions later, don't split off a new version and just make the modifications in place.\n","3. Figure out what changes you need made to your dataset, and do these immediately in one section so you only have one (or a few distinct) datasets to use for analysis.\n","4. Not code: keep a legal pad or notebook with you, and write down your different dataset names as you go."]},{"cell_type":"markdown","metadata":{"id":"VBxhZXsMhlfn"},"source":["# Exercise 6:\n","\n","Use `.loc[]` or `.iloc[]` to get all the following subsets of data from `sector_emissions`\n","\n","1. `\"Agriculture\"` and `\"Heavy Industry` in all years.\n","2. All `Year` values after 2000.\n","3. `Oil and gas` and `Transport` values prior to 2010.\n","4. All columns in 2019, 2020, and 2021."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Drkql6rhlfn"},"outputs":[],"source":["sector_emissions.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OfJCGcgghlfn"},"outputs":[],"source":["# use .loc[] or .iloc[]\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9D8T6B24hlfn"},"source":["# 7. Column Operations\n","\n","Another very important part of Pandas programming is column operations: you can select columns in the `df[\"column\"]` format, and perform arithmetic calculations with them, likely to create new columns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9En9fqzhlfo"},"outputs":[],"source":["regional_emissions[\"1990 to 2005 Difference\"] = regional_emissions[\"2005\"] - regional_emissions[\"1990\"]\n","regional_emissions"]},{"cell_type":"markdown","metadata":{"id":"y3JECC2Ghlfo"},"source":["Here, we subtracted the 1990 values of each province's emissions from the 2005 values, getting the increase or decrease in this time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R1ki-SuDhlfo"},"outputs":[],"source":["regional_emissions[\"multiplication_example\"] = regional_emissions[\"2005\"] * regional_emissions[\"1990\"]\n","\n","regional_emissions[\"division_example\"] = regional_emissions[\"2005\"] / regional_emissions[\"1990\"]\n","\n","regional_emissions"]},{"cell_type":"markdown","metadata":{"id":"fgVTdIIBhlfo"},"source":["As you can see, we can also do multiplication (`*`) and division (`/`). There's no meaningful interpretation to multiplication, though division can show us the percentage change in this time (if we subtract 1 and multiply by 100).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"I4TcMTNXhlfo"},"source":["## Exercise 7:\n","\n","Calculate the percentage change from 1990 to 2005 emissions by taking `\"division_example\"`, subtracting 1, and then multiplying by 100.\n","\n","You can just treat the whole `regional_emissions[\"division_example\"]` as one value for this arithmetic; though the values `1` and `100` aren't the same length, for 1x1 dimension values (like scalars) Pandas just applies them to each row in the column.\n","\n","You can either create a new column for this intermediate step, or wrap the subtraction operation in round brackets `()` to ensure your order of operations is correct."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LsHnHPxxhlfo"},"outputs":[],"source":["# Answer\n"]},{"cell_type":"markdown","metadata":{"id":"ZoqCwww9hlfo"},"source":["# 8. Summary Statistics\n","\n","For some quick summary statistics on a DataFrame's numeric features, you can quickly call `.describe()` after your DataFrame object. It will give you:\n","\n","- count (how many observations)\n","- mean (average)\n","- std (standard deviation)\n","- min (minimum value)\n","- 25%/50%/75% (quartiles)\n","- max (maximum value)\n","\n","These are computed for each column and returned in their own DataFrame as below:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bAaHfGkGhlfo"},"outputs":[],"source":["regional_emissions = regional_emissions[[\"Region\", \"Shortnam\", \"1990\", \"2005\", \"2021\", \"1990 to 2005 Difference\", \"1990 to 2005 percent change\"]]\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Upxb0NI2hlfp"},"source":["# 9.\tOpen-ended, time-permitting exercise\n","\n","Using the techniques we've learned, analyze the `sector_emissions` dataframe. Identify the industries that have seen a decrease in their emissions levels since 2000. Define this however you want!\n","\n","However, you must select your condition, and **explicitly compare the values in the `sector_emissions` dataframe**. There must be a `True` or `False` value returned when classifying!\n","\n","I'll be walking around to give help, but you have to come up with the method yourself."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aeQrt8V-hlfp"},"outputs":[],"source":["sector_emissions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2X76aRHXhlfp"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDikOIPFhlfp"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}